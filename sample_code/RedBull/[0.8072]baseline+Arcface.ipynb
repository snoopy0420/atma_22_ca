{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リスタートに伴い、前回公開したベースラインノートブックを更新しました！\n",
    "\n",
    "主な変更点としては、\n",
    "- 未知選手の検出のためにArcFaceを使用\n",
    "- Pytorch lightningを使用したコードに変更\n",
    "- docstring等を追加\n",
    "\n",
    "（参考までに、リスタート前のコンペにてArcFaceを使用したところ、LBスコア0.9608となりました。  \n",
    "これに加えてたかいとさんがdiscussionで公開した後処理を適用したところ、LBスコア0.9765となりました）\n",
    "\n",
    "このノートブックを実行して訓練すると、val_f1スコアが0.99を超えます。  \n",
    "ですが、LBスコアでは0.8を少し上回る程度となるため、より良い検証方法について検討する必要があります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, RichProgressBar\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from timm.utils import ModelEmaV3\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import F1Score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # パス設定\n",
    "    data_dir: Path = Path(\"/kaggle/input/atmacup22\")\n",
    "    image_dir: Path = Path(\"/kaggle/input/atmacup22/images\")\n",
    "    crop_dir: Path = Path(\"/kaggle/dataset/crops/train\")  # trainデータのクロップ画像の保存先\n",
    "    output_dir: Path = Path(\"/kaggle/working/output\")\n",
    "    use_crops: bool = True  # 高速読み込みのため事前クロップした画像を使用\n",
    "\n",
    "    # モデル設定\n",
    "    model_name: str = \"efficientnet_b0\"\n",
    "    num_classes: int = 11  # label_id 0-10\n",
    "    pretrained: bool = True\n",
    "    img_size: int = 224\n",
    "    embedding_dim: int = 512\n",
    "\n",
    "    # ArcFace設定\n",
    "    arcface_s: float = 30.0\n",
    "    arcface_m: float = 0.5\n",
    "\n",
    "    # 訓練設定\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 8\n",
    "    epochs: int = 20\n",
    "\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    ema_decay: float = 0.995\n",
    "    use_ema: bool = True\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "    # デバイス設定\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "    # WandB設定\n",
    "    use_wandb: bool = False\n",
    "    wandb_project: str = \"atmacup22\"\n",
    "    wandb_run_name: str | None = \"baseline\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing - Crop Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(row: pd.Series, image_dir: Path) -> Path:\n",
    "    \"\"\"データフレームの行から画像パスを生成する\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): メタデータの行（quarter, angle, session, frameを含む）\n",
    "        image_dir (Path): 画像ディレクトリのパス\n",
    "\n",
    "    Returns:\n",
    "        Path: 生成された画像ファイルのパス\n",
    "\n",
    "    Note:\n",
    "        ファイル名の形式: {quarter}__{angle}__{session:02d}__{frame:02d}.jpg\n",
    "    \"\"\"\n",
    "    fname = f\"{row['quarter']}__{row['angle']}__{row['session']:02d}__{row['frame']:02d}.jpg\"\n",
    "    return image_dir / fname\n",
    "\n",
    "\n",
    "def process_single_crop(args: tuple) -> tuple[int, bool]:\n",
    "    \"\"\"単一の画像クロップ処理を実行する\n",
    "\n",
    "    Args:\n",
    "        args (tuple): 処理に必要な引数のタプル\n",
    "            - idx (int): データフレームのインデックス\n",
    "            - row (pd.Series): メタデータの行\n",
    "            - image_dir (Path): 画像ディレクトリのパス\n",
    "            - output_dir (Path): 出力ディレクトリのパス\n",
    "            - padding_ratio (float): パディング比率\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, bool]: (インデックス, 成功フラグ)\n",
    "\n",
    "    Note:\n",
    "        バウンディングボックスにパディングを追加してクロップし、\n",
    "        JPEG品質95%で保存します。\n",
    "    \"\"\"\n",
    "    idx, row, image_dir, output_dir, padding_ratio = args\n",
    "\n",
    "    try:\n",
    "        # 画像パスを取得して画像を読み込み\n",
    "        img_path = get_image_path(row, image_dir)\n",
    "        img = cv2.imread(str(img_path))\n",
    "\n",
    "        if img is None:\n",
    "            return idx, False\n",
    "\n",
    "        # パディング付きのバウンディングボックスを取得\n",
    "        x, y, w, h = int(row[\"x\"]), int(row[\"y\"]), int(row[\"w\"]), int(row[\"h\"])\n",
    "        img_h, img_w = img.shape[:2]\n",
    "\n",
    "        # パディングサイズを計算\n",
    "        pad_w = int(w * padding_ratio)\n",
    "        pad_h = int(h * padding_ratio)\n",
    "\n",
    "        # クロップ範囲を計算（画像境界内に制限）\n",
    "        x1 = max(0, x - pad_w)\n",
    "        y1 = max(0, y - pad_h)\n",
    "        x2 = min(img_w, x + w + pad_w)\n",
    "        y2 = min(img_h, y + h + pad_h)\n",
    "\n",
    "        # 画像をクロップ\n",
    "        crop = img[y1:y2, x1:x2]\n",
    "\n",
    "        # クロップした画像を保存\n",
    "        output_path = output_dir / f\"{idx}.jpg\"\n",
    "        cv2.imwrite(str(output_path), crop, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "\n",
    "        return idx, True\n",
    "    except Exception as e:\n",
    "        print(f\"インデックス {idx} の処理中にエラーが発生: {e}\")\n",
    "        return idx, False\n",
    "\n",
    "\n",
    "def preprocess_crops(\n",
    "    csv_path: Path,\n",
    "    image_dir: Path,\n",
    "    output_dir: Path,\n",
    "    padding_ratio: float = 0.01,\n",
    "    num_workers: int = None,\n",
    "):\n",
    "    \"\"\"全てのバウンディングボックスを事前クロップして保存する\n",
    "\n",
    "    Args:\n",
    "        csv_path (Path): メタデータCSVファイルのパス\n",
    "        image_dir (Path): 元画像が格納されているディレクトリのパス\n",
    "        output_dir (Path): クロップした画像を保存するディレクトリのパス\n",
    "        padding_ratio (float, optional): バウンディングボックスに追加するパディングの比率.\n",
    "                                       デフォルトは0.1（10%）\n",
    "        num_workers (int, optional): 並列処理のワーカー数.\n",
    "                                   Noneの場合はCPUコア数を使用\n",
    "\n",
    "    Note:\n",
    "        - 並列処理を使用して高速化\n",
    "        - 出力ディレクトリが存在しない場合は自動作成\n",
    "        - 失敗したサンプルのインデックスを記録・表示\n",
    "        - クロップした画像は{idx}.jpgの形式で保存\n",
    "    \"\"\"\n",
    "    if num_workers is None:\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    # 出力ディレクトリを作成\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # CSVファイルを読み込み\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"{num_workers}ワーカーで{len(df)}サンプルを処理中...\")\n",
    "\n",
    "    # 並列処理用の引数リストを準備\n",
    "    args_list = [(idx, row, image_dir, output_dir, padding_ratio) for idx, row in df.iterrows()]\n",
    "\n",
    "    # 並列処理で実行\n",
    "    success_count = 0\n",
    "    failed_indices = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # 全てのタスクを投入\n",
    "        futures = {executor.submit(process_single_crop, args): args[0] for args in args_list}\n",
    "\n",
    "        # 完了したタスクから順次結果を取得\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"クロッピング中\"):\n",
    "            idx, success = future.result()\n",
    "            if success:\n",
    "                success_count += 1\n",
    "            else:\n",
    "                failed_indices.append(idx)\n",
    "\n",
    "    # 処理結果を表示\n",
    "    print(f\"完了: {success_count}/{len(df)} クロップを保存\")\n",
    "    if failed_indices:\n",
    "        print(f\"失敗したインデックス: {failed_indices[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform(img_size: int) -> A.Compose:\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_val_transform(img_size: int) -> A.Compose:\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "class PlayerDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        image_dir: Path,\n",
    "        transform: A.Compose,\n",
    "        is_test: bool = False,\n",
    "        cache_images: bool = False,\n",
    "        crop_dir: Path = None,\n",
    "    ):\n",
    "        self.original_indices = df.index.tolist()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.cache_images = cache_images\n",
    "        self.image_cache = {}\n",
    "        self.crop_dir = Path(crop_dir) if crop_dir else None\n",
    "        self.use_crops = crop_dir is not None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_image(self, img_path: Path) -> np.ndarray:\n",
    "        if self.cache_images and str(img_path) in self.image_cache:\n",
    "            return self.image_cache[str(img_path)]\n",
    "\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.cache_images:\n",
    "            self.image_cache[str(img_path)] = img\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        if self.use_crops:\n",
    "            # 事前クロップした画像を直接読み込む（元のインデックスを使用）\n",
    "            original_idx = self.original_indices[idx]\n",
    "            crop_path = self.crop_dir / f\"{original_idx}.jpg\"\n",
    "            crop = self._load_image(crop_path)\n",
    "        else:\n",
    "            # フル画像を読み込んでクロップ\n",
    "            img_path = get_image_path(row, self.image_dir)\n",
    "            img = self._load_image(img_path)\n",
    "\n",
    "            # パディング付きでバウンディングボックスをクロップ\n",
    "            x, y, w, h = int(row[\"x\"]), int(row[\"y\"]), int(row[\"w\"]), int(row[\"h\"])\n",
    "            img_h, img_w = img.shape[:2]\n",
    "\n",
    "            # パディングを追加（bboxサイズの10%）\n",
    "            pad_w = int(w * 0.1)\n",
    "            pad_h = int(h * 0.1)\n",
    "\n",
    "            x1 = max(0, x - pad_w)\n",
    "            y1 = max(0, y - pad_h)\n",
    "            x2 = min(img_w, x + w + pad_w)\n",
    "            y2 = min(img_h, y + h + pad_h)\n",
    "\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "\n",
    "        transformed = self.transform(image=crop)\n",
    "        image = transformed[\"image\"]\n",
    "\n",
    "        result = {\n",
    "            \"image\": image,\n",
    "            \"angle\": row[\"angle\"],\n",
    "        }\n",
    "\n",
    "        if not self.is_test:\n",
    "            result[\"label\"] = torch.tensor(row[\"label_id\"], dtype=torch.long)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        base_dir: str,\n",
    "        transform: A.Compose,\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # rel_pathから直接画像を読み込む\n",
    "        img_path = f\"{self.base_dir}/{row['rel_path']}\"\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 変換を適用\n",
    "        transformed = self.transform(image=img)\n",
    "        image = transformed[\"image\"]\n",
    "\n",
    "        return {\"image\": image}\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    df: pd.DataFrame,\n",
    "    image_dir: Path,\n",
    "    img_size: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    is_train: bool = True,\n",
    "    is_test: bool = False,\n",
    "    crop_dir: Path = None,\n",
    ") -> torch.utils.data.DataLoader:\n",
    "    transform = get_train_transform(img_size) if is_train else get_val_transform(img_size)\n",
    "\n",
    "    dataset = PlayerDataset(\n",
    "        df=df,\n",
    "        image_dir=image_dir,\n",
    "        transform=transform,\n",
    "        is_test=is_test,\n",
    "        crop_dir=crop_dir,\n",
    "    )\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_train,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=is_train,\n",
    "        persistent_workers=num_workers > 0,\n",
    "        prefetch_factor=4 if num_workers > 0 else None,\n",
    "    )\n",
    "\n",
    "\n",
    "class PlayerDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        image_dir: Path,\n",
    "        crop_dir: Path,\n",
    "        img_size: int = 224,\n",
    "        batch_size: int = 128,\n",
    "        num_workers: int = 8,\n",
    "        seed: int = 42,\n",
    "        use_crops: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.crop_dir = crop_dir if use_crops else None\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        train_df = pd.read_csv(self.data_dir / \"train_meta.csv\")\n",
    "        val_mask = train_df[\"quarter\"] >= \"Q2-016\"\n",
    "        self.train_data = train_df[~val_mask]\n",
    "        self.val_data = train_df[val_mask]\n",
    "\n",
    "        print(f\"Train: {len(self.train_data)}, Val: {len(self.val_data)}\")\n",
    "        print(f\"Val quarters: {self.val_data['quarter'].unique()}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return create_dataloader(\n",
    "            self.train_data,\n",
    "            self.image_dir,\n",
    "            self.img_size,\n",
    "            self.batch_size,\n",
    "            self.num_workers,\n",
    "            is_train=True,\n",
    "            crop_dir=self.crop_dir,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return create_dataloader(\n",
    "            self.val_data,\n",
    "            self.image_dir,\n",
    "            self.img_size,\n",
    "            self.batch_size,\n",
    "            self.num_workers,\n",
    "            is_train=False,\n",
    "            crop_dir=self.crop_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceHead(nn.Module):\n",
    "    \"\"\"メトリック学習のためのArcFaceヘッド。\n",
    "\n",
    "    ArcFaceとは、顔認識や人物再識別などのタスクで使用される損失関数の一種です。\n",
    "    正式名称は「Additive Angular Margin Loss」で、特徴量空間において\n",
    "    クラス間の分離をより明確にするために角度マージンを導入します。\n",
    "\n",
    "    主な特徴:\n",
    "    1. 特徴量とクラス重みを単位球面上に正規化\n",
    "    2. コサイン類似度ベースの分類\n",
    "    3. 正解クラスに対してのみ角度マージン（m）を追加\n",
    "    4. スケールファクター（s）で勾配の大きさを調整\n",
    "\n",
    "    これにより、同じクラスの特徴量は密集し、異なるクラス間の特徴量は\n",
    "    より大きな角度で分離されるため、識別性能が向上します。\n",
    "\n",
    "    参考文献: ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
    "    https://arxiv.org/abs/1801.07698\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        s: float = 30.0,\n",
    "        m: float = 0.5,\n",
    "        easy_margin: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.easy_margin = easy_margin\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, x: torch.Tensor, labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        # 特徴量と重みを正規化\n",
    "        x_norm = F.normalize(x, p=2, dim=1)\n",
    "        w_norm = F.normalize(self.weight, p=2, dim=1)\n",
    "\n",
    "        # コサイン類似度\n",
    "        cosine = F.linear(x_norm, w_norm)\n",
    "\n",
    "        if labels is None:\n",
    "            # 推論モード: コサイン類似度を返す\n",
    "            return cosine * self.s\n",
    "\n",
    "        # 訓練モード: 角度マージンを適用\n",
    "        sine = torch.sqrt(1.0 - torch.clamp(cosine * cosine, 0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m  # cos(theta + m)\n",
    "\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        # ワンホットエンコーディング\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "\n",
    "        # ターゲットクラスにのみマージンを適用\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class PlayerEmbeddingModel(nn.Module):\n",
    "    \"\"\"プレイヤー再識別のための埋め込みヘッド付きCNNモデル。\n",
    "\n",
    "    このモデルは、プレイヤーの画像から特徴的な埋め込みベクトルを抽出し、\n",
    "    同じプレイヤーの画像同士は近い埋め込みを、異なるプレイヤーの画像同士は\n",
    "    遠い埋め込みを生成するように学習されます。\n",
    "\n",
    "    アーキテクチャ:\n",
    "    1. バックボーン（CNN）: 画像から高次元特徴量を抽出\n",
    "    2. 埋め込み層: 特徴量を固定次元の埋め込みベクトルに変換\n",
    "    3. ArcFaceヘッド: 訓練時に角度マージンを用いた分類損失を計算\n",
    "\n",
    "    推論時は埋め込みベクトル同士のコサイン類似度を計算して\n",
    "    プレイヤーの同一性を判定します。\n",
    "\n",
    "    Args:\n",
    "        model_name (str): 使用するバックボーンモデル名（timmライブラリ）\n",
    "        embedding_dim (int): 埋め込みベクトルの次元数\n",
    "        num_classes (int): 訓練データに含まれるプレイヤー数\n",
    "        pretrained (bool): ImageNet事前訓練重みを使用するか\n",
    "        arcface_s (float): ArcFaceのスケールパラメータ\n",
    "        arcface_m (float): ArcFaceの角度マージンパラメータ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"resnet18\",\n",
    "        embedding_dim: int = 512,\n",
    "        num_classes: int = 11,\n",
    "        pretrained: bool = True,\n",
    "        arcface_s: float = 30.0,\n",
    "        arcface_m: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 分類器なしのバックボーン\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # 分類器を削除\n",
    "        )\n",
    "\n",
    "        # バックボーンの出力特徴量数を取得\n",
    "        backbone_out = self.backbone.num_features\n",
    "\n",
    "        # 埋め込み層（ArcFace論文のBN-Dropout-FC-BN構成）\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.BatchNorm1d(backbone_out),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(backbone_out, embedding_dim, bias=False),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "        )\n",
    "\n",
    "        # 訓練用のArcFaceヘッド\n",
    "        self.arcface = ArcFaceHead(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=num_classes,\n",
    "            s=arcface_s,\n",
    "            m=arcface_m,\n",
    "        )\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def get_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"正規化された埋め込み特徴量を抽出。\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.embedding(features)\n",
    "        return F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"順伝播。\n",
    "\n",
    "        Args:\n",
    "            x: 入力画像\n",
    "            labels: ArcFaceマージン用のラベル（推論時はNone）\n",
    "\n",
    "        Returns:\n",
    "            訓練時（ラベル提供）: 分類損失用のArcFaceロジット\n",
    "            推論時（ラベルなし）: 正規化された埋め込み\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.embedding(features)\n",
    "\n",
    "        if labels is not None:\n",
    "            # 訓練: ArcFaceロジットを返す\n",
    "            return self.arcface(embedding, labels)\n",
    "        else:\n",
    "            # 推論: 正規化された埋め込みを返す\n",
    "            return F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    model_name: str = \"resnet18\",\n",
    "    num_classes: int = 11,\n",
    "    pretrained: bool = True,\n",
    "    embedding_dim: int = 512,\n",
    "    arcface_s: float = 30.0,\n",
    "    arcface_m: float = 0.5,\n",
    ") -> PlayerEmbeddingModel:\n",
    "    \"\"\"モデルインスタンスを作成。\"\"\"\n",
    "    return PlayerEmbeddingModel(\n",
    "        model_name=model_name,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_classes=num_classes,\n",
    "        pretrained=pretrained,\n",
    "        arcface_s=arcface_s,\n",
    "        arcface_m=arcface_m,\n",
    "    )\n",
    "\n",
    "\n",
    "class PlayerModule(pl.LightningModule):\n",
    "    \"\"\"ArcFaceを使用したプレイヤー再識別のためのLightningモジュール。\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"resnet18\",\n",
    "        num_classes: int = 11,\n",
    "        pretrained: bool = True,\n",
    "        embedding_dim: int = 512,\n",
    "        arcface_s: float = 30.0,\n",
    "        arcface_m: float = 0.5,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 1e-4,\n",
    "        epochs: int = 20,\n",
    "        ema_decay: float = 0.9998,\n",
    "        use_ema: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = create_model(\n",
    "            model_name=model_name,\n",
    "            num_classes=num_classes,\n",
    "            pretrained=pretrained,\n",
    "            embedding_dim=embedding_dim,\n",
    "            arcface_s=arcface_s,\n",
    "            arcface_m=arcface_m,\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "        self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_decay = ema_decay\n",
    "        self.model_ema = None\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        \"\"\"モデルがデバイスに移動された後にEMAを初期化。\"\"\"\n",
    "        if self.use_ema and self.model_ema is None:\n",
    "            self.model_ema = ModelEmaV3(\n",
    "                self.model,\n",
    "                decay=self.ema_decay,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        return self.model(x, labels)\n",
    "\n",
    "    def get_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"埋め込みを抽出（利用可能な場合はEMAモデルを使用）。\"\"\"\n",
    "        if self.use_ema and self.model_ema is not None:\n",
    "            return self.model_ema.module.get_embedding(x)\n",
    "        return self.model.get_embedding(x)\n",
    "\n",
    "    def forward_ema(self, x: torch.Tensor, labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"EMAモデルを使用した順伝播。\"\"\"\n",
    "        if self.model_ema is not None:\n",
    "            return self.model_ema.module(x, labels)\n",
    "        return self.model(x, labels)\n",
    "\n",
    "    def on_before_zero_grad(self, optimizer):\n",
    "        \"\"\"各最適化ステップ後にEMAを更新。\"\"\"\n",
    "        if self.use_ema and self.model_ema is not None:\n",
    "            self.model_ema.update(self.model)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        # ArcFaceマージンのためにラベル付きで順伝播\n",
    "        outputs = self(images, labels)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        self.train_f1(preds, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_f1\", self.train_f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        # 検証にはEMAモデルを使用（適切なマージンのためにラベル付き）\n",
    "        if self.use_ema and self.model_ema is not None:\n",
    "            outputs = self.forward_ema(images, labels)\n",
    "        else:\n",
    "            outputs = self(images, labels)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        self.val_f1(preds, labels)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_f1\", self.val_f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.epochs)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_prototypes(\n",
    "    model: PlayerModule,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    num_classes: int,\n",
    "    device: str,\n",
    ") -> dict:\n",
    "    \"\"\"訓練データからクラスプロトタイプ（平均埋め込み）を計算する\n",
    "\n",
    "    プロトタイプとは、各クラスの代表的な特徴ベクトルのことです。\n",
    "    具体的には、同じクラスに属する全ての埋め込みベクトルの平均を取り、\n",
    "    正規化したものがプロトタイプになります。これにより、各クラスの\n",
    "    「典型的な」特徴を表現するベクトルを得ることができます。\n",
    "\n",
    "    推論時には、新しいサンプルの埋め込みベクトルと各クラスの\n",
    "    プロトタイプとの類似度（コサイン類似度など）を計算することで、\n",
    "    最も近いクラスを予測することができます。\n",
    "\n",
    "    Returns:\n",
    "        以下を含む辞書:\n",
    "        - prototypes: [num_classes, embedding_dim] クラス重心のテンソル\n",
    "        - all_embeddings: [N, embedding_dim] 全埋め込みのテンソル\n",
    "        - all_labels: [N] 全ラベルのテンソル\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"埋め込み計算中\"):\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        # 埋め込みを抽出\n",
    "        embeddings = model.get_embedding(images)\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # クラスプロトタイプを計算（クラスごとの埋め込みの平均）\n",
    "    embedding_dim = all_embeddings.shape[1]\n",
    "    prototypes = torch.zeros(num_classes, embedding_dim)\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        mask = all_labels == c\n",
    "        if mask.sum() > 0:\n",
    "            class_embeddings = all_embeddings[mask]\n",
    "            # 平均を正規化して単位ベクトルプロトタイプを取得\n",
    "            prototype = class_embeddings.mean(dim=0)\n",
    "            prototypes[c] = torch.nn.functional.normalize(prototype, p=2, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"prototypes\": prototypes,\n",
    "        \"all_embeddings\": all_embeddings,\n",
    "        \"all_labels\": all_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings(\n",
    "    model: PlayerModule,\n",
    "    loader: DataLoader,\n",
    "    device: str,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"画像から埋め込みベクトルを抽出する。\n",
    "\n",
    "    埋め込みベクトルは、画像の特徴を高次元空間（通常512次元）で表現したベクトルです。\n",
    "    同じプレイヤーの画像は類似した埋め込みベクトルを持ち、異なるプレイヤーの画像は\n",
    "    異なる埋め込みベクトルを持つように学習されています。これにより、ベクトル間の\n",
    "    距離やコサイン類似度を計算することで、プレイヤーの同一性を判定できます。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Extracting embeddings\"):\n",
    "        images = batch[\"image\"].to(device, non_blocking=True)\n",
    "        emb = model.get_embedding(images)\n",
    "        embeddings.append(emb.cpu())\n",
    "\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def predict_with_prototypes(\n",
    "    embeddings: torch.Tensor,\n",
    "    prototypes: torch.Tensor,\n",
    "    threshold: float = 0.5,\n",
    ") -> list[int]:\n",
    "    \"\"\"プロトタイプとのコサイン類似度でラベルを予測する。\n",
    "\n",
    "    Args:\n",
    "        embeddings: テスト埋め込み（正規化済み）[N, embedding_dim]\n",
    "        prototypes: クラスプロトタイプ（正規化済み）[num_classes, embedding_dim]\n",
    "        threshold: 最小類似度閾値。最大類似度がこの値未満の場合、\n",
    "                   -1（訓練データに存在しない不明プレイヤー）を予測する。\n",
    "\n",
    "    Returns:\n",
    "        list[int]: 予測ラベルのリスト（不明プレイヤーは-1）\n",
    "    \"\"\"\n",
    "    # コサイン類似度を計算（両方とも既に正規化済み）\n",
    "    similarities = F.linear(embeddings, prototypes)  # [N, num_classes]\n",
    "    max_sims, max_indices = similarities.max(dim=1)\n",
    "\n",
    "    # 最大類似度が閾値未満の場合、-1（不明プレイヤー）を予測\n",
    "    predictions = []\n",
    "    for sim, idx in zip(max_sims.tolist(), max_indices.tolist()):\n",
    "        if sim < threshold:\n",
    "            predictions.append(-1)\n",
    "        else:\n",
    "            predictions.append(idx)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データを事前クロップ\n",
    "preprocess_crops(\n",
    "    csv_path=cfg.data_dir / \"train_meta.csv\",\n",
    "    image_dir=cfg.image_dir,\n",
    "    output_dir=cfg.crop_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(cfg.seed)\n",
    "\n",
    "# データモジュール\n",
    "dm = PlayerDataModule(\n",
    "    data_dir=cfg.data_dir,\n",
    "    image_dir=cfg.image_dir,\n",
    "    crop_dir=cfg.crop_dir,\n",
    "    img_size=cfg.img_size,\n",
    "    batch_size=cfg.batch_size,\n",
    "    num_workers=cfg.num_workers,\n",
    "    seed=cfg.seed,\n",
    "    use_crops=cfg.use_crops,\n",
    ")\n",
    "\n",
    "# モデル\n",
    "model = PlayerModule(\n",
    "    model_name=cfg.model_name,\n",
    "    num_classes=cfg.num_classes,\n",
    "    pretrained=cfg.pretrained,\n",
    "    embedding_dim=cfg.embedding_dim,\n",
    "    arcface_s=cfg.arcface_s,\n",
    "    arcface_m=cfg.arcface_m,\n",
    "    lr=cfg.lr,\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    ema_decay=cfg.ema_decay,\n",
    "    use_ema=cfg.use_ema,\n",
    "    epochs=cfg.epochs,\n",
    ")\n",
    "\n",
    "# コールバック\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=cfg.output_dir,\n",
    "    filename=\"best_model\",\n",
    "    monitor=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    enable_version_counter=False,\n",
    ")\n",
    "progress_bar = RichProgressBar(\n",
    "    theme=RichProgressBarTheme(\n",
    "        description=\"cyan\",\n",
    "        progress_bar=\"blue\",\n",
    "        progress_bar_finished=\"bright_blue\",\n",
    "        progress_bar_pulse=\"#0080FF\",\n",
    "        batch_progress=\"cyan\",\n",
    "        time=\"grey82\",\n",
    "        processing_speed=\"grey82\",\n",
    "        metrics=\"grey82\",\n",
    "    )\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_callback, progress_bar]\n",
    "\n",
    "if cfg.use_wandb:\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=cfg.wandb_project,\n",
    "        name=cfg.wandb_run_name,\n",
    "        save_dir=\"/tmp\",\n",
    "        log_model=True,\n",
    "    )\n",
    "    wandb_logger.log_hyperparams(cfg.__dict__)\n",
    "    callbacks.append(LearningRateMonitor(logging_interval=\"epoch\"))\n",
    "else:\n",
    "    wandb_logger = None\n",
    "\n",
    "# トレーナー\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=cfg.epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    logger=wandb_logger if cfg.use_wandb else False,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_progress_bar=True,\n",
    "    deterministic=True,\n",
    "    default_root_dir=\"/tmp\",\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compute Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n訓練データからクラスプロトタイプを計算中...\")\n",
    "\n",
    "# チェックポイントを手動で読み込み、EMAキーをフィルタリング\n",
    "checkpoint = torch.load(checkpoint_callback.best_model_path, map_location=cfg.device, weights_only=False)\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "# model_emaキーをフィルタリング（推論には不要）\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"model_ema.\")}\n",
    "\n",
    "# 推論用にEMAなしの新しいモデルを作成\n",
    "best_model = PlayerModule(\n",
    "    model_name=cfg.model_name,\n",
    "    num_classes=cfg.num_classes,\n",
    "    pretrained=False,  # チェックポイントから読み込むため事前訓練重みは不要\n",
    "    embedding_dim=cfg.embedding_dim,\n",
    "    arcface_s=cfg.arcface_s,\n",
    "    arcface_m=cfg.arcface_m,\n",
    "    lr=cfg.lr,\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    ema_decay=cfg.ema_decay,\n",
    "    use_ema=False,  # 推論時はEMAを無効化\n",
    "    epochs=cfg.epochs,\n",
    ")\n",
    "best_model.load_state_dict(filtered_state_dict, strict=False)\n",
    "best_model = best_model.to(cfg.device)\n",
    "best_model.eval()\n",
    "\n",
    "# 全訓練データ用のデータローダーを作成（拡張なし）\n",
    "train_df = pd.read_csv(cfg.data_dir / \"train_meta.csv\")\n",
    "train_loader = create_dataloader(\n",
    "    train_df,\n",
    "    cfg.image_dir,\n",
    "    cfg.img_size,\n",
    "    cfg.batch_size,\n",
    "    cfg.num_workers,\n",
    "    is_train=False,  # 拡張なし\n",
    "    crop_dir=cfg.crop_dir,\n",
    ")\n",
    "\n",
    "# プロトタイプを計算して保存\n",
    "prototype_data = compute_prototypes(\n",
    "    best_model,\n",
    "    train_loader,\n",
    "    cfg.num_classes,\n",
    "    cfg.device,\n",
    ")\n",
    "\n",
    "# プロトタイプを保存\n",
    "prototype_path = cfg.output_dir / \"prototypes.pt\"\n",
    "torch.save(prototype_data, prototype_path)\n",
    "print(f\"プロトタイプを保存しました: {prototype_path}\")\n",
    "print(f\"プロトタイプ形状: {prototype_data['prototypes'].shape}\")\n",
    "print(f\"総埋め込み数: {prototype_data['all_embeddings'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = str(cfg.data_dir)  # /kaggle/input/atmacup22\n",
    "\n",
    "test_df = pd.read_csv(cfg.data_dir / \"test_meta.csv\")\n",
    "print(f\"テストサンプル数: {len(test_df)}\")\n",
    "\n",
    "# チェックポイントを読み込む\n",
    "ckpt_path = cfg.output_dir / \"best_model.ckpt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=cfg.device, weights_only=False)\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "# model_emaキーを除外\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"model_ema.\")}\n",
    "\n",
    "# モデルを作成\n",
    "inference_model = PlayerModule(\n",
    "    model_name=cfg.model_name,\n",
    "    num_classes=cfg.num_classes,\n",
    "    pretrained=False,\n",
    "    embedding_dim=cfg.embedding_dim,\n",
    "    arcface_s=cfg.arcface_s,\n",
    "    arcface_m=cfg.arcface_m,\n",
    "    lr=cfg.lr,\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    ema_decay=cfg.ema_decay,\n",
    "    use_ema=False,\n",
    "    epochs=cfg.epochs,\n",
    ")\n",
    "inference_model.load_state_dict(filtered_state_dict, strict=False)\n",
    "inference_model = inference_model.to(cfg.device)\n",
    "inference_model.eval()\n",
    "print(f\"モデルを読み込みました: {ckpt_path}\")\n",
    "\n",
    "# プロトタイプを読み込む\n",
    "prototype_path = cfg.output_dir / \"prototypes.pt\"\n",
    "prototype_data = torch.load(prototype_path, map_location=\"cpu\", weights_only=True)\n",
    "prototypes = prototype_data[\"prototypes\"]\n",
    "print(f\"プロトタイプを読み込みました: {prototypes.shape}\")\n",
    "\n",
    "# データセットとデータローダーを作成\n",
    "transform = get_val_transform(cfg.img_size)\n",
    "test_dataset = TestDataset(test_df, base_dir, transform)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,  # 順番を保持\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"\\nテスト埋め込みを抽出中...\")\n",
    "embeddings = extract_embeddings(inference_model, test_loader, cfg.device)\n",
    "print(f\"埋め込み形状: {embeddings.shape}\")\n",
    "\n",
    "print(\"\\nプロトタイプとのコサイン類似度で予測中...\")\n",
    "predictions = predict_with_prototypes(embeddings, prototypes, threshold=0.5)\n",
    "\n",
    "submission = pd.DataFrame({\"label_id\": predictions})\n",
    "submission_path = cfg.output_dir / \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"提出ファイルを保存しました: {submission_path}\")\n",
    "\n",
    "print(\"\\n予測分布:\")\n",
    "print(pd.Series(predictions).value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

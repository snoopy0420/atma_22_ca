"""
ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ç¾¤
éå»ã‚³ãƒ³ãƒšã®æ§‹æˆã«å¾“ã£ãŸè¨­è¨ˆ
"""
import datetime
import logging
import sys
import os
import json
import numpy as np
import pandas as pd
import joblib
from typing import Dict, List, Tuple, Any, Optional
from sklearn.model_selection import (
    KFold, 
    StratifiedKFold, 
    GroupKFold, 
    StratifiedGroupKFold
)
from sklearn.metrics import f1_score

sys.path.append(os.path.abspath('..'))
from configs.config import *


class Util:
    """ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""

    @classmethod
    def dump(cls, value, path):
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¿å­˜"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        joblib.dump(value, path, compress=True)

    @classmethod
    def load(cls, path):
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        return joblib.load(path)

    @classmethod
    def dump_json(cls, value, path):
        """JSONã‚’ä¿å­˜"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(value, f, indent=4, ensure_ascii=False)

    @classmethod
    def load_json(cls, path):
        """JSONã‚’èª­ã¿è¾¼ã¿"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    @classmethod
    def dump_df_pickle(cls, df, path):
        """DataFrameã‚’pickleã§ä¿å­˜"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        df.to_pickle(path)

    @classmethod
    def load_df_pickle(cls, path):
        """DataFrameã‚’pickleã‹ã‚‰èª­ã¿è¾¼ã¿"""
        return pd.read_pickle(path)

    @classmethod
    def load_feature(cls, file_name):
        """ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        file_name = file_name if file_name.endswith('.pkl') else file_name + ".pkl"
        return pd.read_pickle(os.path.join(DIR_FEATURE, file_name))
    
    @classmethod
    def save_submission(cls, submission: pd.DataFrame, run_name: str, suffix: str = "", logger=None) -> str:
        """æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
        
        Note: Submissionã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨ã‚’æ¨å¥¨
        """
        return Submission.save(submission, run_name, suffix, logger)


class Submission:
    """æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def save(submission: pd.DataFrame, run_name: str, logger=None) -> str:
        """æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        
        Args:
            submission: æå‡ºç”¨DataFrameï¼ˆlabel_idåˆ—ã‚’æŒã¤ï¼‰
            run_name: å®Ÿè¡Œå
            suffix: ãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            logger: ãƒ­ã‚¬ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            ä¿å­˜å…ˆã®ãƒ‘ã‚¹
        
        Examples:
            >>> submission = pd.DataFrame({'label_id': predictions})
            >>> Submission.save(submission, 'resnet50_knn', 'tuned')
            'data/submission/submission_resnet50_knn_tuned_20251218_143022.csv'
        """
        from datetime import datetime
        
        if Submission.validate(submission):

            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"submission_{run_name}_{timestamp}.csv"
            save_path = os.path.join(DIR_SUBMISSIONS, filename)
            
            # ä¿å­˜
            os.makedirs(DIR_SUBMISSIONS, exist_ok=True)
            submission.to_csv(save_path, index=False, header=True)
            
            logger.info(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {save_path}")
            
    
    @staticmethod
    def validate(submission: pd.DataFrame) -> bool:
        """æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        
        Args:
            submission: æå‡ºç”¨DataFrame
            expected_length: æœŸå¾…ã•ã‚Œã‚‹è¡Œæ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            æ¤œè¨¼çµæœï¼ˆTrue: OK, False: NGï¼‰
        """
        sample_submission = pd.read_csv(FILE_SAMPLE_SUBMISSION)
        
        # ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
        if submission.columns != sample_submission.columns:
            print(f"âŒ ã‚«ãƒ©ãƒ åã‚¨ãƒ©ãƒ¼: {submission.columns}ï¼ˆæœŸå¾…: {sample_submission.columns}ï¼‰")
            return False
        
        # # é•·ã•ãƒã‚§ãƒƒã‚¯
        # expected_length = sample_submission.shape[0]
        # if len(submission) != expected_length:
        #     print(f"âŒ è¡Œæ•°ã‚¨ãƒ©ãƒ¼: {len(submission)}è¡Œï¼ˆæœŸå¾…: {expected_length}è¡Œï¼‰")
        #     return False
        
        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        if not pd.api.types.is_integer_dtype(submission['label_id']):
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿å‹ã‚¨ãƒ©ãƒ¼: label_idåˆ—ã¯æ•´æ•°å‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
            return False
        
        
        print(f"âœ… ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆåŠŸ: {len(submission)}è¡Œ")
        return True


class Logger:
    """ãƒ­ã‚®ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""

    def __init__(self, path):
        """
        Args:
            path: ãƒ­ã‚°å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        os.makedirs(path, exist_ok=True)
        
        self.general_logger = logging.getLogger(os.path.join(path, 'general'))
        self.result_logger = logging.getLogger(os.path.join(path, 'result'))
        
        stream_handler = logging.StreamHandler()
        file_general_handler = logging.FileHandler(os.path.join(path, 'general.log'))
        file_result_handler = logging.FileHandler(os.path.join(path, 'result.log'))
        
        if len(self.general_logger.handlers) == 0:
            self.general_logger.addHandler(stream_handler)
            self.general_logger.addHandler(file_general_handler)
            self.general_logger.setLevel(logging.INFO)
            self.result_logger.addHandler(stream_handler)
            self.result_logger.addHandler(file_result_handler)
            self.result_logger.setLevel(logging.INFO)

    def info(self, message):
        """æ™‚åˆ»ä»˜ãã§ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¨ãƒ­ã‚°ã«å‡ºåŠ›"""
        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))

    def result(self, message):
        """çµæœãƒ­ã‚°ã«å‡ºåŠ›"""
        self.result_logger.info(message)

    def result_ltsv(self, dic):
        """çµæœã‚’LTSVå½¢å¼ã§å‡ºåŠ›"""
        self.result(self.to_ltsv(dic))

    def result_scores(self, run_name, scores):
        """CVã‚¹ã‚³ã‚¢ã‚’å‡ºåŠ›"""
        dic = dict()
        dic['run_name'] = run_name
        dic['score_mean'] = np.mean(scores)
        dic['score_std'] = np.std(scores)
        for i, score in enumerate(scores):
            dic[f'score{i}'] = score
        self.result(self.to_ltsv(dic))

    def section_start(self, title: str, width: int = 80):
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹ãƒ­ã‚°"""
        self.info("")
        self.info("="*width)
        self.info(title)
        self.info("="*width)

    def section_end(self, title: str = "Completed", width: int = 80):
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†ãƒ­ã‚°"""
        self.info("="*width)
        self.info(title)
        self.info("="*width)

    def fold_start(self, fold_idx: int, n_folds: int, width: int = 80):
        """Foldé–‹å§‹ãƒ­ã‚°"""
        self.info("")
        self.info("="*width)
        self.info(f"Fold {fold_idx} / {n_folds}")
        self.info("="*width)

    def fold_result(self, fold_idx: int, score: float, metric_name: str = "Macro F1", train_size: int = None, valid_size: int = None):
        """Foldçµæœãƒ­ã‚°"""
        if train_size and valid_size:
            self.info(f"  Train: {train_size:,}, Valid: {valid_size:,}")
        self.info(f"  {metric_name}: {score:.6f}")

    def cv_summary(self, scores: list, width: int = 80):
        """CVã‚µãƒãƒªãƒ¼ãƒ­ã‚°"""
        self.info("")
        self.info("="*width)
        self.info("CV Results Summary")
        self.info("="*width)
        for i, score in enumerate(scores):
            self.info(f"  Fold {i}: {score:.6f}")
        self.info(f"  Mean: {np.mean(scores):.6f} (+/- {np.std(scores):.6f})")

    def now_string(self):
        """ç¾åœ¨æ™‚åˆ»ã®æ–‡å­—åˆ—"""
        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

    def to_ltsv(self, dic):
        """è¾æ›¸ã‚’LTSVå½¢å¼ã«å¤‰æ›"""
        return '\t'.join(['{}:{}'.format(key, value) for key, value in dic.items()])


class Metric:
    """è©•ä¾¡æŒ‡æ¨™ã‚¯ãƒ©ã‚¹"""

    @classmethod
    def macro_f1(cls, y_true, y_pred, labels: Optional[List[int]] = None):
        """
        Macro F1ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        
        Args:
            y_true: æ­£è§£ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            labels: è©•ä¾¡å¯¾è±¡ã®ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ¨å®šï¼‰
        Returns:
            Macro F1ã‚¹ã‚³ã‚¢
        """
        if labels is None:
            # -1ï¼ˆunknownï¼‰ã‚‚å«ã‚ã¦è©•ä¾¡
            labels = sorted(set(y_true) | set(y_pred))
        
        score = f1_score(y_true, y_pred, labels=labels, average='macro', zero_division=0)
        return score

    @classmethod
    def my_metric(cls, y_true, y_pred):
        """ã‚³ãƒ³ãƒšç”¨ã®è©•ä¾¡æŒ‡æ¨™ï¼ˆMacro F1ï¼‰"""
        return cls.macro_f1(y_true, y_pred)


class Validation:
    """CVåˆ†å‰²ã¨ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def create_validator(method: str, n_splits: int = 5, **kwargs):
        """
        CVæ‰‹æ³•ã‚’é¸æŠã—ã¦validatorã‚’ç”Ÿæˆ
        
        Args:
            method: CVæ‰‹æ³•ã®ç¨®é¡
                - 'kfold': KFold
                - 'stratified': StratifiedKFold
                - 'group': GroupKFold (æ¨å¥¨: ãƒªãƒ¼ã‚¯é˜²æ­¢)
                - 'stratified_group': StratifiedGroupKFold
            n_splits: Foldæ•°
            **kwargs: å„validatorå›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                - shuffle: ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®æœ‰ç„¡ (default: True)
                - random_state: ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (default: 42)
        
        Returns:
            sklearn cross-validator
        """
        shuffle = kwargs.get('shuffle', True)
        random_state = kwargs.get('random_state', 42)
        
        if method == 'kfold':
            return KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)
        
        elif method == 'stratified':
            return StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)
        
        elif method == 'group':
            # GroupKFold: ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§ãƒªãƒ¼ã‚¯ãªã—ï¼ˆå±¤åŒ–ãªã—ï¼‰
            return GroupKFold(n_splits=n_splits)
        
        elif method == 'stratified_group':
            # StratifiedGroupKFold: ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§ãƒªãƒ¼ã‚¯ãªã— + å±¤åŒ–ã‚’è©¦ã¿ã‚‹
            return StratifiedGroupKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)
        
        else:
            raise ValueError(f"Unknown CV method: {method}. Choose from ['kfold', 'stratified', 'group', 'stratified_group']")
    
    
    @staticmethod
    def check_group_leak(validator, X: pd.DataFrame, y: np.ndarray, 
                        groups: np.ndarray, verbose: bool = True) -> Dict[str, Any]:
        """
        ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ãƒªãƒ¼ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            validator: sklearn cross-validator
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé…åˆ—
            groups: ã‚°ãƒ«ãƒ¼ãƒ—é…åˆ—
            verbose: è©³ç´°å‡ºåŠ›ã®æœ‰ç„¡
        
        Returns:
            Dict containing:
                - has_leak: bool (ãƒªãƒ¼ã‚¯ã®æœ‰ç„¡)
                - fold_results: List[Dict] (å„Foldã®çµæœ)
        """
        fold_results = []
        has_leak = False
        
        if verbose:
            print("="*80)
            print("ğŸ” CV Group Leak Check")
            print("="*80)
        
        for fold_idx, (train_idx, valid_idx) in enumerate(validator.split(X, y, groups)):
            train_groups = set(groups[train_idx])
            valid_groups = set(groups[valid_idx])
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            overlap = train_groups & valid_groups
            fold_has_leak = len(overlap) > 0
            has_leak = has_leak or fold_has_leak
            
            leak_status = "âŒ LEAK!" if fold_has_leak else "âœ… No leak"
            
            # é¸æ‰‹åˆ†å¸ƒ
            train_labels = set(y[train_idx])
            valid_labels = set(y[valid_idx])
            
            # å„é¸æ‰‹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
            train_label_counts = pd.Series(y[train_idx]).value_counts()
            valid_label_counts = pd.Series(y[valid_idx]).value_counts()
            
            fold_result = {
                'fold': fold_idx,
                'train_samples': len(train_idx),
                'valid_samples': len(valid_idx),
                'train_groups': len(train_groups),
                'valid_groups': len(valid_groups),
                'overlap_groups': len(overlap),
                'has_leak': fold_has_leak,
                'train_labels': len(train_labels),
                'valid_labels': len(valid_labels),
                'overlap_labels': len(train_labels & valid_labels),
                'train_label_min': train_label_counts.min(),
                'train_label_max': train_label_counts.max(),
                'valid_label_min': valid_label_counts.min(),
                'valid_label_max': valid_label_counts.max(),
            }
            fold_results.append(fold_result)
            
            if verbose:
                print(f"\nFold {fold_idx}: {leak_status}")
                print(f"  Train: {len(train_idx):5,} samples, {len(train_groups):3d} groups")
                print(f"  Valid: {len(valid_idx):5,} samples, {len(valid_groups):3d} groups")
                print(f"  Overlap groups: {len(overlap)}")
                print(f"  Players - Train: {len(train_labels)}, Valid: {len(valid_labels)}, Overlap: {len(train_labels & valid_labels)}")
                print(f"  Label balance (train): min={train_label_counts.min()}, max={train_label_counts.max()}")
                print(f"  Label balance (valid): min={valid_label_counts.min()}, max={valid_label_counts.max()}")
        
        if verbose:
            print("\n" + "="*80)
            if has_leak:
                print("âŒ LEAK DETECTED!")
            else:
                print("âœ… No Leakage - CV Strategy is Valid")
            print("="*80)
        
        return {
            'has_leak': has_leak,
            'fold_results': fold_results
        }
    
    
    @staticmethod
    def get_cv_statistics(validator, X: pd.DataFrame, y: np.ndarray, 
                         groups: np.ndarray) -> pd.DataFrame:
        """
        CVåˆ†å‰²ã®çµ±è¨ˆæƒ…å ±ã‚’DataFrameã¨ã—ã¦å–å¾—
        
        Args:
            validator: sklearn cross-validator
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé…åˆ—
            groups: ã‚°ãƒ«ãƒ¼ãƒ—é…åˆ—
        
        Returns:
            pd.DataFrame: å„Foldã®çµ±è¨ˆæƒ…å ±
        """
        fold_stats = []
        
        for fold_idx, (train_idx, valid_idx) in enumerate(validator.split(X, y, groups)):
            train_groups = set(groups[train_idx])
            valid_groups = set(groups[valid_idx])
            
            fold_stats.append({
                'fold': fold_idx,
                'train_samples': len(train_idx),
                'valid_samples': len(valid_idx),
                'train_groups': len(train_groups),
                'valid_groups': len(valid_groups),
                'overlap_groups': len(train_groups & valid_groups),
            })
        
        return pd.DataFrame(fold_stats)
    
    
    @staticmethod
    def split_by_index(df: pd.DataFrame, train_idx: np.ndarray, 
                      valid_idx: np.ndarray) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        DataFrameã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§åˆ†å‰²
        
        Args:
            df: åˆ†å‰²å¯¾è±¡ã®DataFrame
            train_idx: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            valid_idx: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: (train_df, valid_df)
        """
        train_df = df.iloc[train_idx].reset_index(drop=True)
        valid_df = df.iloc[valid_idx].reset_index(drop=True)
        return train_df, valid_df
    
    
    @staticmethod
    def log_fold_result(fold_idx: int, train_size: int, valid_size: int, 
                       score: float, metric_name: str = "Macro F1"):
        """
        Foldçµæœã®ãƒ­ã‚°å‡ºåŠ›
        
        Args:
            fold_idx: Foldç•ªå·
            train_size: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
            valid_size: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
            score: ã‚¹ã‚³ã‚¢
            metric_name: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å
        """
        print(f"\n{'='*60}")
        print(f"Fold {fold_idx} Results")
        print(f"{'='*60}")
        print(f"  Train samples: {train_size:,}")
        print(f"  Valid samples: {valid_size:,}")
        print(f"  {metric_name}: {score:.6f}")
    
    
    @staticmethod
    def log_cv_summary(scores: List[float], metric_name: str = "Macro F1"):
        """
        CVå…¨ä½“ã®ã‚µãƒãƒªãƒ¼ãƒ­ã‚°å‡ºåŠ›
        
        Args:
            scores: å„Foldã®ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
            metric_name: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å
        """
        print(f"\n{'='*60}")
        print(f"Cross Validation Summary")
        print(f"{'='*60}")
        print(f"  {metric_name} - Mean: {np.mean(scores):.6f}")
        print(f"  {metric_name} - Std:  {np.std(scores):.6f}")
        print(f"  Fold scores: {[f'{s:.6f}' for s in scores]}")
        print(f"{'='*60}")

"""
ãƒ¢ãƒ‡ãƒ«æ€§èƒ½åˆ†æãƒ„ãƒ¼ãƒ«
æ··åŒè¡Œåˆ—ã€ã‚¯ãƒ©ã‚¹åˆ¥F1ã€èª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–
"""
import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, f1_score
from typing import Optional

sys.path.append(os.path.abspath('..'))
from configs.config import *


class ModelAnalyzer:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©³ç´°åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, logger=None):
        self.logger = logger
        
    def log(self, msg):
        """ãƒ­ã‚°å‡ºåŠ›"""
        if self.logger:
            self.logger.info(msg)
        else:
            print(msg)
    
    def analyze_predictions(self, 
                          y_true: np.ndarray, 
                          y_pred: np.ndarray,
                          output_dir: str,
                          prefix: str = ""):
        """äºˆæ¸¬çµæœã®åŒ…æ‹¬çš„ãªåˆ†æ
        
        Args:
            y_true: æ­£è§£ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            prefix: ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        self._basic_statistics(y_true, y_pred)
        
        # 2. ã‚¯ãƒ©ã‚¹åˆ¥F1ã‚¹ã‚³ã‚¢
        self._class_wise_f1(y_true, y_pred, output_dir, prefix)
        
        # 3. æ··åŒè¡Œåˆ—
        self._plot_confusion_matrix(y_true, y_pred, output_dir, prefix)
        
        # 4. èª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        self._error_analysis(y_true, y_pred, output_dir, prefix)
        
        # 5. unknownäºˆæ¸¬ã®åˆ†æ
        self._unknown_analysis(y_true, y_pred)
    
    def _basic_statistics(self, y_true, y_pred):
        """åŸºæœ¬çµ±è¨ˆæƒ…å ±"""
        self.log("\n" + "="*80)
        self.log("ğŸ“Š äºˆæ¸¬çµ±è¨ˆ")
        self.log("="*80)
        
        # å…¨ä½“ç²¾åº¦
        accuracy = (y_true == y_pred).mean()
        macro_f1 = f1_score(y_true, y_pred, average='macro')
        
        self.log(f"ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(y_true):,}")
        self.log(f"æ­£è§£ç‡: {accuracy:.4f}")
        self.log(f"Macro F1: {macro_f1:.4f}")
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        self.log("\nã€æ­£è§£ãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒã€‘")
        unique, counts = np.unique(y_true, return_counts=True)
        for label, count in zip(unique, counts):
            self.log(f"  ã‚¯ãƒ©ã‚¹ {label:2d}: {count:5d} ({count/len(y_true)*100:5.2f}%)")
        
        self.log("\nã€äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒã€‘")
        unique, counts = np.unique(y_pred, return_counts=True)
        for label, count in zip(unique, counts):
            self.log(f"  ã‚¯ãƒ©ã‚¹ {label:2d}: {count:5d} ({count/len(y_pred)*100:5.2f}%)")
    
    def _class_wise_f1(self, y_true, y_pred, output_dir, prefix):
        """ã‚¯ãƒ©ã‚¹åˆ¥F1ã‚¹ã‚³ã‚¢"""
        self.log("\n" + "="*80)
        self.log("ğŸ“ˆ ã‚¯ãƒ©ã‚¹åˆ¥æ€§èƒ½")
        self.log("="*80)
        
        # ã‚¯ãƒ©ã‚¹åˆ¥F1ã‚’è¨ˆç®—
        labels = np.unique(np.concatenate([y_true, y_pred]))
        report = classification_report(
            y_true, y_pred, 
            labels=labels,
            target_names=[f"Class {l}" for l in labels],
            output_dict=True,
            zero_division=0
        )
        
        # DataFrameåŒ–ã—ã¦ä¿å­˜
        df_report = pd.DataFrame(report).transpose()
        csv_path = os.path.join(output_dir, f"{prefix}class_report.csv")
        df_report.to_csv(csv_path)
        self.log(f"ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {csv_path}")
        
        # F1ã‚¹ã‚³ã‚¢ã®ä½ã„ã‚¯ãƒ©ã‚¹TOP5ã‚’è¡¨ç¤º
        class_f1 = {label: report[f"Class {label}"]["f1-score"] 
                    for label in labels if f"Class {label}" in report}
        sorted_f1 = sorted(class_f1.items(), key=lambda x: x[1])
        
        self.log("\nã€F1ã‚¹ã‚³ã‚¢ãŒä½ã„ã‚¯ãƒ©ã‚¹ TOP5ã€‘")
        for label, f1 in sorted_f1[:5]:
            support = report[f"Class {label}"]["support"]
            self.log(f"  ã‚¯ãƒ©ã‚¹ {label:2d}: F1={f1:.4f} (ã‚µãƒ³ãƒ—ãƒ«æ•°: {support})")
        
        self.log("\nã€F1ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã‚¯ãƒ©ã‚¹ TOP5ã€‘")
        for label, f1 in sorted_f1[-5:][::-1]:
            support = report[f"Class {label}"]["support"]
            self.log(f"  ã‚¯ãƒ©ã‚¹ {label:2d}: F1={f1:.4f} (ã‚µãƒ³ãƒ—ãƒ«æ•°: {support})")
    
    def _plot_confusion_matrix(self, y_true, y_pred, output_dir, prefix):
        """æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–"""
        labels = np.unique(np.concatenate([y_true, y_pred]))
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        
        # æ­£è¦åŒ–ç‰ˆã‚‚ä½œæˆ
        cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)
        cm_normalized = np.nan_to_num(cm_normalized)  # ã‚¼ãƒ­é™¤ç®—å¯¾ç­–
        
        # ãƒ—ãƒ­ãƒƒãƒˆ
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # ç”Ÿã®æ··åŒè¡Œåˆ—
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=labels, yticklabels=labels, ax=axes[0])
        axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)
        axes[0].set_xlabel('Predicted', fontsize=12)
        axes[0].set_ylabel('True', fontsize=12)
        
        # æ­£è¦åŒ–ç‰ˆ
        sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                   xticklabels=labels, yticklabels=labels, ax=axes[1])
        axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14)
        axes[1].set_xlabel('Predicted', fontsize=12)
        axes[1].set_ylabel('True', fontsize=12)
        
        plt.tight_layout()
        fig_path = os.path.join(output_dir, f"{prefix}confusion_matrix.png")
        plt.savefig(fig_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        self.log(f"\næ··åŒè¡Œåˆ—ä¿å­˜: {fig_path}")
    
    def _error_analysis(self, y_true, y_pred, output_dir, prefix):
        """èª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        self.log("\n" + "="*80)
        self.log("ğŸ” èª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
        self.log("="*80)
        
        # èª¤åˆ†é¡ã®ã¿æŠ½å‡º
        errors = y_true != y_pred
        if errors.sum() == 0:
            self.log("èª¤åˆ†é¡ãªã—ï¼ˆå®Œç’§ãªäºˆæ¸¬ï¼‰")
            return
        
        self.log(f"èª¤åˆ†é¡æ•°: {errors.sum():,} / {len(y_true):,} ({errors.mean()*100:.2f}%)")
        
        # èª¤åˆ†é¡ãƒšã‚¢ã®é »åº¦
        error_pairs = list(zip(y_true[errors], y_pred[errors]))
        from collections import Counter
        pair_counts = Counter(error_pairs)
        
        self.log("\nã€é »å‡ºèª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³ TOP10ã€‘")
        for (true_label, pred_label), count in pair_counts.most_common(10):
            self.log(f"  {true_label:2d} â†’ {pred_label:2d}: {count:4d}å›")
        
        # èª¤åˆ†é¡ãƒšã‚¢ã‚’DataFrameã§ä¿å­˜
        df_errors = pd.DataFrame({
            'true_label': y_true[errors],
            'pred_label': y_pred[errors]
        })
        csv_path = os.path.join(output_dir, f"{prefix}error_pairs.csv")
        df_errors['true_label'].value_counts().to_csv(csv_path.replace('.csv', '_by_true.csv'))
        df_errors['pred_label'].value_counts().to_csv(csv_path.replace('.csv', '_by_pred.csv'))
        
        self.log(f"èª¤åˆ†é¡ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {csv_path}")
    
    def _unknown_analysis(self, y_true, y_pred):
        """unknown (-1) äºˆæ¸¬ã®åˆ†æ"""
        self.log("\n" + "="*80)
        self.log("â“ unknownäºˆæ¸¬ã®åˆ†æ")
        self.log("="*80)
        
        # unknownäºˆæ¸¬
        pred_unknown = y_pred == -1
        true_unknown = y_true == -1
        
        self.log(f"unknownäºˆæ¸¬æ•°: {pred_unknown.sum():,} ({pred_unknown.mean()*100:.2f}%)")
        self.log(f"çœŸã®unknownæ•°: {true_unknown.sum():,} ({true_unknown.mean()*100:.2f}%)")
        
        if pred_unknown.sum() > 0:
            # unknownäºˆæ¸¬æ™‚ã®æ­£è§£ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            self.log("\nã€unknownäºˆæ¸¬æ™‚ã®çœŸã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã€‘")
            true_when_pred_unknown = y_true[pred_unknown]
            unique, counts = np.unique(true_when_pred_unknown, return_counts=True)
            for label, count in zip(unique, counts):
                self.log(f"  ã‚¯ãƒ©ã‚¹ {label:2d}: {count:4d} ({count/pred_unknown.sum()*100:5.2f}%)")
        
        if true_unknown.sum() > 0:
            # çœŸã®unknownæ™‚ã®äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            self.log("\nã€çœŸã®unknownã«å¯¾ã™ã‚‹äºˆæ¸¬åˆ†å¸ƒã€‘")
            pred_when_true_unknown = y_pred[true_unknown]
            unique, counts = np.unique(pred_when_true_unknown, return_counts=True)
            for label, count in zip(unique, counts):
                self.log(f"  ã‚¯ãƒ©ã‚¹ {label:2d}: {count:4d} ({count/true_unknown.sum()*100:5.2f}%)")


def analyze_oof_predictions(oof_path: str, train_df: pd.DataFrame, logger=None):
    """OOFäºˆæ¸¬çµæœã‚’åˆ†æ
    
    Args:
        oof_path: OOFäºˆæ¸¬çµæœã®pklãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        train_df: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«å«ã‚€ï¼‰
        logger: ãƒ­ã‚¬ãƒ¼
    """
    from src.util import Util
    
    # OOFäºˆæ¸¬èª­ã¿è¾¼ã¿
    df_oof = Util.load_df_pickle(oof_path)
    
    # æ­£è§£ãƒ©ãƒ™ãƒ«ã¨ãƒãƒ¼ã‚¸
    df_merged = train_df.loc[df_oof.index, ['label_id']].copy()
    df_merged['pred'] = df_oof['label_id'].values
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = ModelAnalyzer(logger)
    output_dir = os.path.dirname(oof_path)
    analyzer.analyze_predictions(
        y_true=df_merged['label_id'].values,
        y_pred=df_merged['pred'].values,
        output_dir=output_dir,
        prefix="oof_"
    )


if __name__ == "__main__":
    # ä½¿ç”¨ä¾‹
    import pandas as pd
    from src.util import Logger
    
    logger = Logger()
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_df = pd.read_csv(f'{DIR_INPUT}/atmaCup22_2nd_meta/train_meta.csv')
    
    # æœ€æ–°ã®OOFäºˆæ¸¬ã‚’åˆ†æ
    import glob
    oof_files = glob.glob(f'{DIR_MODEL}/*/va_pred.pkl')
    if oof_files:
        latest_oof = max(oof_files, key=os.path.getmtime)
        logger.info(f"æœ€æ–°OOFäºˆæ¸¬ã‚’åˆ†æ: {latest_oof}")
        analyze_oof_predictions(latest_oof, train_df, logger)
    else:
        logger.warning("OOFäºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
